# PatternBook SDLC v2.1 - Updated Proposal with Agent-Based Validation

**Date:** 2025-11-01
**Type:** Workflow Reliability & Efficiency Update
**Priority:** Critical - Addresses workflow-breaking issues
**Estimated Effort:** 28-30 hours (was 16h in original proposal)
**Status:** Ready for Implementation

---

## Executive Summary

This proposal addresses critical reliability issues in PatternBook SDLC v2.0 while **preserving the core "friction as feature" philosophy**. No shortcuts or bypasses are introduced. Instead, we make the correct path reliable and efficient.

**Key Innovation:** **Agent-Based Validation Pattern**

Instead of complex human approval processes or blind trust, v2.1 introduces specialized validation agents that provide rigorous, objective review while remaining simple for solo developers:

- Tech-lead agent reviews emergency requests
- Refactor-validator agent verifies coverage drops
- Compression agent manages context intelligently

This maintains governance rigor through AI while keeping the workflow simple.

---

## Changes from Original v2.1 Proposal

Based on architecture review and user feedback:

### ‚úÖ Kept from Original

- StateManager centralization with atomic writes and backups
- Agent retry mechanism with checkpoints
- Feedback loop limits (5 for planning, 3 for implementation)
- Task type detection and optimization
- Parallel CI validation
- Task-parallel readiness in STATE.json

### üîÑ Modified from Original

1. **Side-Effect Tracking:** Changed from hardcoded migration types to **plugin-based generic system**
2. **Context Compression:** Changed from unspecified to **LLM-based with dedicated SKILL.md**
3. **Emergency Approval:** Changed from human multi-approval to **tech-lead agent review**
4. **Coverage Validation:** Changed from keyword matching to **refactor-validator agent**
5. **Frontend Metrics:** Simplified to basic checks, complex metrics marked as future
6. **Task Types:** Dropped "component" type, kept 5 types (docs/config/backend/frontend/fullstack)

### ‚ûï New Additions

1. **Validation Agent System:** New architectural pattern for governance
2. **Cross-Platform File Locking:** Using `filelock` library instead of platform-specific `fcntl`
3. **Agent Failure Fallback:** Human override when validation agents crash
4. **Compression Skill:** New reference document with LLM prompts
5. **Side-Effect Plugin API:** Extensible for future migration strategies

---

## PART 1: Critical Reliability Fixes

### 1.1 StateManager Resilience ‚úÖ APPROVED

**Implementation:**

```python
# NEW FILE: .claude/skills/docs-stories/scripts/state_manager.py
from pathlib import Path
import json
import shutil
from filelock import FileLock  # Cross-platform locking
import time

class StateManager:
    """Centralized STATE.json management with atomic writes and locking"""

    MAX_BACKUPS = 3
    MAX_LOCK_WAIT = 10  # seconds

    @staticmethod
    def update_state(task_id: str, updates: dict) -> dict:
        """
        Update STATE.json atomically with locking and backups.

        Args:
            task_id: Task identifier (e.g., "TASK-001")
            updates: Dictionary of updates to merge into state

        Returns:
            Updated state dictionary

        Raises:
            FileNotFoundError: If STATE.json doesn't exist
            TimeoutError: If cannot acquire lock
            ValidationError: If state validation fails
        """
        state_file = Path(f".claude/tasks/{task_id}/STATE.json")
        lock_file = state_file.with_suffix('.lock')
        backup_file = state_file.with_suffix('.bak')

        # Acquire exclusive lock
        lock = FileLock(str(lock_file), timeout=StateManager.MAX_LOCK_WAIT)

        try:
            with lock:
                # 1. Create backup
                if state_file.exists():
                    shutil.copy(state_file, backup_file)

                # 2. Load current state
                state = json.loads(state_file.read_text())

                # 3. Merge updates
                state = StateManager._deep_merge(state, updates)

                # 4. Validate schema
                StateManager._validate_schema(state)

                # 5. Atomic write
                temp_file = state_file.with_suffix('.tmp')
                temp_file.write_text(json.dumps(state, indent=2))
                temp_file.replace(state_file)

                # 6. Log mutation
                StateManager._log_mutation(task_id, updates)

                # 7. Rotate backups
                StateManager._rotate_backups(task_id)

                return state

        except Timeout:
            raise TimeoutError(
                f"Could not acquire lock for {task_id} after {StateManager.MAX_LOCK_WAIT}s. "
                f"Another process may be updating state."
            )

    @staticmethod
    def _deep_merge(base: dict, updates: dict) -> dict:
        """Deep merge with type safety"""
        result = base.copy()

        for key, value in updates.items():
            if key in result:
                # Type safety check
                if type(result[key]) != type(value):
                    raise TypeError(
                        f"Type mismatch for '{key}': "
                        f"{type(result[key]).__name__} vs {type(value).__name__}"
                    )

                # Recursive merge for dicts
                if isinstance(value, dict):
                    result[key] = StateManager._deep_merge(result[key], value)
                else:
                    result[key] = value
            else:
                result[key] = value

        return result

    @staticmethod
    def _validate_schema(state: dict):
        """Validate state structure"""
        required_fields = ['format_version', 'task_id', 'phase', 'created_at']

        for field in required_fields:
            if field not in state:
                raise ValidationError(f"Missing required field: {field}")

        # Format version check
        if not state['format_version'].startswith('2.'):
            raise ValidationError(f"Unsupported format version: {state['format_version']}")

        # Task ID format
        import re
        if not re.match(r'^TASK-\d{3}$', state['task_id']):
            raise ValidationError(f"Invalid task_id format: {state['task_id']}")

    @staticmethod
    def _log_mutation(task_id: str, updates: dict):
        """Log state mutations for audit trail"""
        log_file = Path(f".claude/tasks/{task_id}/state.log")

        entry = {
            'timestamp': datetime.now().isoformat(),
            'updates': updates,
            'caller': StateManager._get_caller()
        }

        with open(log_file, 'a') as f:
            f.write(json.dumps(entry) + '\n')

    @staticmethod
    def _rotate_backups(task_id: str):
        """Keep last MAX_BACKUPS backups"""
        backup_dir = Path(f".claude/tasks/{task_id}")
        backups = sorted(backup_dir.glob('STATE.*.bak'), key=lambda p: p.stat().st_mtime)

        # Remove old backups
        while len(backups) > StateManager.MAX_BACKUPS:
            oldest = backups.pop(0)
            oldest.unlink()

    @staticmethod
    def _get_caller():
        """Get calling script name for logging"""
        import inspect
        frame = inspect.currentframe().f_back.f_back.f_back
        return Path(frame.f_code.co_filename).name

class ValidationError(Exception):
    """State validation failed"""
    pass
```

**Update All Scripts:**

All 33 scripts must be updated to use StateManager:

```python
# BEFORE (Direct JSON manipulation):
state_file = Path(f".claude/tasks/{task_id}/STATE.json")
state = json.loads(state_file.read_text())
state['phase'] = 'IMPLEMENTATION'
state_file.write_text(json.dumps(state, indent=2))

# AFTER (Using StateManager):
from state_manager import StateManager
StateManager.update_state(task_id, {'phase': 'IMPLEMENTATION'})
```

**New Command: /task-recover**

````markdown
# NEW FILE: .claude/commands/task-recover.md

You are helping recover a corrupted task.

## Prerequisites

- Task ID must be provided as argument

## Steps

1. Check if STATE.json is corrupted:
   ```bash
   python .claude/skills/docs-stories/scripts/task_validate_state.py {TASK_ID}
   ```
````

2. If corrupted, list available backups:

   ```bash
   ls -lt .claude/tasks/{TASK_ID}/STATE.*.bak
   ```

3. Restore from most recent backup:

   ```bash
   python .claude/skills/docs-stories/scripts/task_recover.py {TASK_ID}
   ```

4. Validate recovered state:

   ```bash
   python .claude/skills/docs-stories/scripts/task_validate_state.py {TASK_ID}
   ```

5. Report recovery results to user

````

---

### 1.2 Agent Failure Recovery with Side-Effect Tracking ‚úÖ APPROVED

**Key Decision:** Generic plugin-based system (no hardcoded migration types)

**Implementation:**

```python
# NEW FILE: .claude/skills/docs-stories/scripts/side_effect_tracker.py
from pathlib import Path
import json
from typing import Callable, Dict, Any, List
from datetime import datetime

class SideEffectTracker:
    """
    Generic side-effect tracking with undo callbacks.

    Agents register side effects (DB changes, git commits, API calls)
    with undo callbacks. On retry, all effects are rolled back.

    Design: Plugin-based, no assumptions about specific tools (Alembic, etc.)
    """

    def __init__(self, task_id: str, agent_name: str):
        self.task_id = task_id
        self.agent_name = agent_name
        self.checkpoint_dir = Path(f".claude/tasks/{task_id}/checkpoints")
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.effects: List[Dict[str, Any]] = []

    def record_side_effect(self,
                          effect_type: str,
                          description: str,
                          data: Dict[str, Any],
                          undo_script: str = None):
        """
        Record a side effect with optional undo script.

        Args:
            effect_type: Category (e.g., 'database', 'git', 'external_api')
            description: Human-readable description
            data: Effect-specific data (e.g., migration_id, commit_hash)
            undo_script: Optional path to script that undoes this effect
        """
        effect = {
            'timestamp': datetime.now().isoformat(),
            'type': effect_type,
            'description': description,
            'data': data,
            'undo_script': undo_script
        }

        self.effects.append(effect)
        self._persist()

    def rollback_all(self) -> List[str]:
        """
        Attempt to rollback all side effects in reverse order.

        Returns:
            List of warnings/errors encountered
        """
        warnings = []

        for effect in reversed(self.effects):
            try:
                self._rollback_effect(effect)
            except Exception as e:
                warning = f"Failed to rollback {effect['type']}: {e}"
                warnings.append(warning)
                print(f"‚ö†Ô∏è {warning}")

        return warnings

    def _rollback_effect(self, effect: Dict[str, Any]):
        """Rollback a single effect"""
        if effect.get('undo_script'):
            # Execute undo script
            import subprocess
            result = subprocess.run(
                ['python', effect['undo_script']] + self._extract_args(effect),
                capture_output=True,
                text=True
            )

            if result.returncode != 0:
                raise RuntimeError(f"Undo script failed: {result.stderr}")
        else:
            # Log that manual intervention may be needed
            print(f"‚ö†Ô∏è No undo script for {effect['type']}: {effect['description']}")
            print(f"   Data: {effect['data']}")
            print(f"   Manual rollback may be required")

    def _persist(self):
        """Save effects to checkpoint file"""
        checkpoint_file = self.checkpoint_dir / f"{self.agent_name}-effects.json"
        checkpoint_file.write_text(json.dumps(self.effects, indent=2))

    @classmethod
    def load(cls, task_id: str, agent_name: str) -> 'SideEffectTracker':
        """Load existing tracker from checkpoint"""
        tracker = cls(task_id, agent_name)
        checkpoint_file = tracker.checkpoint_dir / f"{agent_name}-effects.json"

        if checkpoint_file.exists():
            tracker.effects = json.loads(checkpoint_file.read_text())

        return tracker

    def _extract_args(self, effect: Dict[str, Any]) -> List[str]:
        """Extract command-line args from effect data"""
        # Convert data dict to command args
        args = []
        for key, value in effect['data'].items():
            args.extend([f"--{key}", str(value)])
        return args


# Example usage by agents:
def agent_implementation_example():
    """How an agent would use SideEffectTracker"""

    tracker = SideEffectTracker.load(task_id, 'dev-backend')

    # Example 1: Database migration (future, when migration process defined)
    # tracker.record_side_effect(
    #     effect_type='database_migration',
    #     description='Create users table',
    #     data={'migration_id': '20251101_create_users'},
    #     undo_script='.claude/skills/docs-stories/scripts/undo_migration.py'
    # )

    # Example 2: Git commit
    result = subprocess.run(['git', 'commit', '-m', 'Add feature'], capture_output=True)
    commit_hash = subprocess.run(['git', 'rev-parse', 'HEAD'],
                                capture_output=True, text=True).stdout.strip()

    tracker.record_side_effect(
        effect_type='git_commit',
        description='Committed feature implementation',
        data={'commit_hash': commit_hash},
        undo_script='.claude/skills/docs-stories/scripts/undo_git_commit.py'
    )

    # Example 3: External API call (no undo possible)
    tracker.record_side_effect(
        effect_type='external_api',
        description='Sent deployment notification to Slack',
        data={'channel': '#deployments', 'message_id': 'msg_123'},
        undo_script=None  # Cannot undo external calls
    )

    tracker._persist()
````

**Undo Scripts:**

```python
# NEW FILE: .claude/skills/docs-stories/scripts/undo_git_commit.py
#!/usr/bin/env python3
"""Undo a git commit (soft reset)"""
import subprocess
import sys

def main():
    if len(sys.argv) < 2:
        print("Usage: undo_git_commit.py --commit_hash <hash>")
        sys.exit(1)

    commit_hash = sys.argv[2]

    # Soft reset (keeps changes in working directory)
    result = subprocess.run(['git', 'reset', '--soft', f'{commit_hash}^'])

    if result.returncode != 0:
        print(f"Failed to undo commit {commit_hash}")
        sys.exit(1)

    print(f"‚úÖ Undid commit {commit_hash} (changes preserved in working directory)")

if __name__ == "__main__":
    main()
```

**Integration with SubagentStop Hook:**

```python
# MODIFY: .claude/hooks/subagent_stop.py
from side_effect_tracker import SideEffectTracker

def main():
    # ... existing validation ...

    if not is_work_complete(agent_context):
        retry_count = state.get('agent_retries', {}).get(agent_name, 0)

        if retry_count < 3:
            print(f"‚ö†Ô∏è {agent_name} incomplete. Retry {retry_count + 1}/3")

            # NEW: Rollback side effects before retry
            tracker = SideEffectTracker.load(task_id, agent_name)
            warnings = tracker.rollback_all()

            if warnings:
                print(f"‚ö†Ô∏è Rollback warnings:")
                for warning in warnings:
                    print(f"   - {warning}")

            # Save checkpoint
            save_checkpoint(agent_name, agent_context)

            # Update retry count
            StateManager.update_state(task_id, {
                f'agent_retries.{agent_name}': retry_count + 1,
                'last_retry_reason': 'incomplete_work',
                'rollback_warnings': warnings
            })

            # Signal main LLM to retry
            return 2  # Special exit code for retry
        else:
            # Create recovery document
            create_recovery_guide(task_id, agent_name, agent_context)
            print(f"‚ùå {agent_name} failed 3 times. Manual intervention required.")
            print(f"   Recovery guide: .claude/tasks/{task_id}/recovery.md")
            return 1  # Block progress
```

---

### 1.3 Feedback Loop Limits ‚úÖ APPROVED

**Implementation:**

```python
# MODIFY: .claude/commands/task-plan.md

MAX_PLANNING_ITERATIONS = 5

# In the command flow:
iteration = state.get('planning_iterations', 0)

# Progressive warnings
if iteration == 3:
    print("üìä Planning feedback: 3/5 iterations used")
elif iteration == 4:
    print("‚ö†Ô∏è Planning feedback: 4/5 iterations used (1 remaining)")
elif iteration >= MAX_PLANNING_ITERATIONS:
    print(f"üõë Planning iteration limit reached ({MAX_PLANNING_ITERATIONS})")
    print()
    print("The plan has been refined 5 times. Continuing further may indicate:")
    print("  ‚Ä¢ Story requirements are unclear or too broad")
    print("  ‚Ä¢ Multiple approaches being explored (consider splitting story)")
    print("  ‚Ä¢ Perfectionism (plan doesn't need to be perfect)")
    print()
    print("Options:")
    print("  1. Auto-approve current plan (RECOMMENDED)")
    print("  2. Continue with 3 more iterations (extend limit)")
    print("  3. Abort and revise story requirements")

    # Ask user
    choice = ask_user_via_main_llm()

    if choice == 1:
        StateManager.update_state(task_id, {
            'planning_iterations': iteration,
            'plan_auto_approved': True,
            'plan_warning': 'Iteration limit reached - auto-approved'
        })
        # Proceed to implementation
    elif choice == 2:
        # Extend limit
        MAX_PLANNING_ITERATIONS += 3
        print(f"Extended limit to {MAX_PLANNING_ITERATIONS} iterations")
    else:
        # Abort
        print("Aborting planning. Revise story requirements before retrying.")
        sys.exit(1)
```

**Same pattern for implementation:**

```python
# MODIFY: .claude/commands/task-implement.md

MAX_IMPLEMENTATION_ITERATIONS = 3  # Lower than planning

# Implementation refinement should be rarer
# If agent comes back 3 times, something is wrong with the plan
```

---

### 1.4 Context Size Management with LLM Compression ‚úÖ APPROVED

**New Skill Reference Document:**

```markdown
# NEW FILE: .claude/skills/docs-stories/references/context-compression.md

# Context Compression Strategy

## Overview

When context exceeds MAX_CONTEXT_TOKENS (50,000), the system uses LLM-based
compression to preserve essential information while reducing token count.

## Compression Rules

### Priority Levels

**CRITICAL (Never Compress):**

- story content
- acceptance_criteria
- current_phase
- task_id

**IMPORTANT (Truncate Structure):**

- plan (keep acceptance criteria mapping, summarize implementation details)
- test_results (keep pass/fail summary, truncate verbose output)
- commits (keep last 10, truncate messages to first line)

**OPTIONAL (Drop if Needed):**

- previous_errors (keep last 3)
- git_history (drop entirely if needed)
- verbose logs

### Compression Prompts

When summarizing **plan**:
```

Summarize this implementation plan, preserving:

- All acceptance criteria references
- High-level approach (1-2 sentences)
- Major technical decisions
- Dependencies and blockers

Drop:

- Verbose explanations
- Step-by-step details
- Code examples

Output format: Bullet points, max 500 tokens.

```

When summarizing **test_results**:
```

Summarize these test results, preserving:

- Total tests run/passed/failed
- Names of failing tests (if any)
- Critical error messages

Drop:

- Full stack traces
- Passing test details
- Verbose output

Output format: Structured summary, max 300 tokens.

```

## Agent-Specific Context

### Backend Agent
Essential fields:
- api_spec (API endpoints and contracts)
- db_schema (database schema)
- plan, acceptance_criteria

Can drop:
- git_history, previous_errors

### Frontend Agent
Essential fields:
- design_spec (UI mockups, design tokens)
- component_api (component interfaces)
- plan, acceptance_criteria

Can drop:
- git_history, api_spec details

### DevOps Agent
Essential fields:
- infrastructure_spec
- deployment_config
- plan, acceptance_criteria

Can drop:
- code details, git_history

## Fallback Behavior

If compression fails to get under MAX_CONTEXT_TOKENS:
1. Try dropping all OPTIONAL fields
2. Try truncating IMPORTANT fields more aggressively
3. If still over: Raise ContextTooLargeError with guidance to split story
```

**Implementation Script:**

````python
# NEW FILE: .claude/skills/docs-stories/scripts/compress_context.py
#!/usr/bin/env python3
"""
LLM-based context compression using prompts from context-compression.md
"""
from pathlib import Path
import json
import anthropic
from typing import Dict, Any

# Load compression prompts from SKILL.md
COMPRESSION_PROMPTS_FILE = Path(__file__).parent.parent / 'references' / 'context-compression.md'

class ContextManager:
    MAX_CONTEXT_TOKENS = 50000

    # Critical fields never compressed
    ESSENTIAL_FIELDS = ['story', 'acceptance_criteria', 'current_phase', 'task_id']

    # Important fields: truncate structure
    TRUNCATE_FIELDS = {
        'plan': 500,
        'test_results': 300,
        'commits': 10  # Keep last 10
    }

    # Optional fields: drop if needed
    OPTIONAL_FIELDS = ['previous_errors', 'git_history', 'verbose_logs']

    def __init__(self, agent_name: str = None):
        self.agent_name = agent_name
        self.client = anthropic.Anthropic()

    def prepare_context(self, task_id: str, phase: str) -> Dict[str, Any]:
        """Load and compress context for agent"""

        # 1. Load full context
        context = self._load_full_context(task_id, phase)

        # 2. Apply agent-specific priorities
        if self.agent_name:
            context = self._apply_agent_priorities(context)

        # 3. Estimate tokens
        token_count = self._estimate_tokens(context)

        # 4. Compress if needed
        if token_count > self.MAX_CONTEXT_TOKENS:
            print(f"‚ö†Ô∏è Context too large: {token_count} tokens (max {self.MAX_CONTEXT_TOKENS})")
            context = self._compress_context(context)
            token_count = self._estimate_tokens(context)
            print(f"‚úÖ Compressed to {token_count} tokens")

        return context

    def _compress_context(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Multi-phase compression"""
        compressed = {}

        # Phase 1: Keep all essentials
        for field in self.ESSENTIAL_FIELDS:
            if field in context:
                compressed[field] = context[field]

        # Phase 2: Compress important fields using LLM
        for field, max_tokens in self.TRUNCATE_FIELDS.items():
            if field in context:
                compressed[field] = self._llm_compress(
                    field,
                    context[field],
                    max_tokens
                )

        # Phase 3: Check if under limit
        if self._estimate_tokens(compressed) <= self.MAX_CONTEXT_TOKENS:
            return compressed

        # Phase 4: Drop optional fields one by one
        for field in self.OPTIONAL_FIELDS:
            if self._estimate_tokens(compressed) <= self.MAX_CONTEXT_TOKENS:
                break
            compressed.pop(field, None)

        # Phase 5: Last resort - error if still over
        final_tokens = self._estimate_tokens(compressed)
        if final_tokens > self.MAX_CONTEXT_TOKENS * 1.1:
            raise ContextTooLargeError(
                f"Cannot compress to {self.MAX_CONTEXT_TOKENS} tokens.\n"
                f"Essential fields alone are {final_tokens} tokens.\n"
                f"Story may be too complex - consider splitting into multiple stories."
            )

        return compressed

    def _llm_compress(self, field_name: str, content: Any, max_tokens: int) -> str:
        """Use LLM to compress field content"""

        # Load compression prompt for this field
        prompt = self._load_compression_prompt(field_name)

        # Call Claude Haiku (fast and cheap for compression)
        response = self.client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=max_tokens,
            messages=[{
                "role": "user",
                "content": f"{prompt}\n\nContent to compress:\n{content}"
            }]
        )

        return response.content[0].text

    def _load_compression_prompt(self, field_name: str) -> str:
        """Load compression prompt from context-compression.md"""
        prompts = self._parse_compression_prompts()
        return prompts.get(field_name, "Summarize this content concisely.")

    def _parse_compression_prompts(self) -> Dict[str, str]:
        """Parse compression prompts from markdown file"""
        content = COMPRESSION_PROMPTS_FILE.read_text()
        prompts = {}

        # Simple parser: look for "When summarizing **field**:" sections
        import re
        pattern = r'When summarizing \*\*(\w+)\*\*:\s*```(.*?)```'
        matches = re.findall(pattern, content, re.DOTALL)

        for field, prompt in matches:
            prompts[field] = prompt.strip()

        return prompts

    def _apply_agent_priorities(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Apply agent-specific context priorities"""

        agent_priorities = {
            'dev-backend': {
                'essential': ['api_spec', 'db_schema'],
                'drop': ['git_history', 'design_spec']
            },
            'dev-frontend': {
                'essential': ['design_spec', 'component_api'],
                'drop': ['git_history', 'api_spec']
            },
            'devops-infra': {
                'essential': ['infrastructure_spec', 'deployment_config'],
                'drop': ['git_history']
            }
        }

        priorities = agent_priorities.get(self.agent_name, {})

        # Add agent-specific essential fields
        for field in priorities.get('essential', []):
            if field in context:
                self.ESSENTIAL_FIELDS.append(field)

        # Drop agent-specific unnecessary fields
        for field in priorities.get('drop', []):
            context.pop(field, None)

        return context

    def _estimate_tokens(self, content: Any) -> int:
        """Estimate token count using tiktoken"""
        try:
            import tiktoken
            enc = tiktoken.encoding_for_model("claude-3-sonnet-20240229")
            return len(enc.encode(json.dumps(content)))
        except ImportError:
            # Fallback: rough estimate (1 token ‚âà 4 chars)
            return len(json.dumps(content)) // 4

    def _load_full_context(self, task_id: str, phase: str) -> Dict[str, Any]:
        """Load all context files for task"""
        task_dir = Path(f".claude/tasks/{task_id}")

        context = {}

        # Load STATE.json
        state_file = task_dir / "STATE.json"
        if state_file.exists():
            context.update(json.loads(state_file.read_text()))

        # Load phase-specific files
        phase_files = {
            'PLANNING': ['story.md', 'plan.md'],
            'IMPLEMENTATION': ['story.md', 'plan.md', 'api_spec.md'],
            'TESTING': ['test_results.json'],
            'VALIDATION': ['validation_results.json']
        }

        for file in phase_files.get(phase, []):
            file_path = task_dir / file
            if file_path.exists():
                field_name = file_path.stem
                if file_path.suffix == '.json':
                    context[field_name] = json.loads(file_path.read_text())
                else:
                    context[field_name] = file_path.read_text()

        return context


class ContextTooLargeError(Exception):
    """Context cannot be compressed to acceptable size"""
    pass
````

---

## PART 2: Efficiency Optimizations

### 2.1 Task Types for Smart Workflow ‚úÖ APPROVED

**Simplified to 5 types** (dropped 'component' type per user feedback):

```python
# MODIFY: .claude/skills/docs-stories/scripts/task_new.py

TASK_TYPES = {
    'docs': {
        'patterns': ['*.md', '*.txt', '*.rst', 'docs/**', '.sdlc-workflow/**'],
        'skip_phases': ['research', 'backend', 'frontend'],
        'required_agents': [],
        'validation': ['spell_check', 'link_check', 'markdown_lint']
    },
    'config': {
        'patterns': ['*.json', '*.yml', '*.yaml', '*.toml', '.env*', '*config*'],
        'skip_phases': ['research'],
        'required_agents': ['devops-infra'],
        'validation': ['syntax_check', 'schema_validation']
    },
    'backend': {
        'patterns': ['apps/server/**', 'tests/backend/**', '**.py'],
        'skip_phases': [],
        'required_agents': ['dev-backend'],
        'validation': ['pytest', 'mypy', 'coverage']
    },
    'frontend': {
        'patterns': ['apps/frontend/**', 'tests/frontend/**', '**.svelte', '**.tsx'],
        'skip_phases': [],
        'required_agents': ['dev-frontend'],
        'validation': ['vitest', 'tsc', 'coverage', 'basic_a11y']
    },
    'fullstack': {
        'patterns': [],  # Determined by story content
        'skip_phases': [],
        'required_agents': ['dev-backend', 'dev-frontend'],
        'validation': ['all']
    }
}

def detect_task_type(story_id: str) -> str:
    """
    Auto-detect task type with override support.

    Priority:
    1. Explicit type in story frontmatter
    2. File paths in acceptance criteria
    3. Content analysis
    4. Default to 'fullstack'
    """
    story = load_story(story_id)

    # Priority 1: Explicit override in story
    if 'type' in story:
        type_value = story['type']
        if type_value in TASK_TYPES:
            return type_value
        else:
            print(f"‚ö†Ô∏è Unknown task type '{type_value}' in story, using auto-detection")

    # Priority 2: File path analysis
    criteria = story.get('acceptance_criteria', '')
    files_mentioned = extract_file_paths(criteria)

    if files_mentioned:
        # Check if all files match a single type
        for task_type, config in TASK_TYPES.items():
            if task_type == 'fullstack':
                continue

            patterns = config['patterns']
            if all(matches_any_pattern(f, patterns) for f in files_mentioned):
                return task_type

    # Priority 3: Content analysis
    keywords = {
        'backend': ['API', 'endpoint', 'database', 'migration', 'FastAPI'],
        'frontend': ['UI', 'component', 'page', 'Svelte', 'form'],
        'docs': ['documentation', 'README', 'guide'],
        'config': ['configuration', 'environment', 'settings']
    }

    criteria_lower = criteria.lower()
    matched_types = []

    for task_type, keyword_list in keywords.items():
        if any(kw.lower() in criteria_lower for kw in keyword_list):
            matched_types.append(task_type)

    # If only one type matched, use it
    if len(matched_types) == 1:
        return matched_types[0]

    # If both backend and frontend matched, it's fullstack
    if 'backend' in matched_types and 'frontend' in matched_types:
        return 'fullstack'

    # Priority 4: Default to fullstack (safest)
    return 'fullstack'


def matches_any_pattern(file_path: str, patterns: List[str]) -> bool:
    """Check if file matches any pattern"""
    from fnmatch import fnmatch
    return any(fnmatch(file_path, pattern) for pattern in patterns)


def extract_file_paths(text: str) -> List[str]:
    """Extract file paths from text"""
    import re
    # Match common file path patterns
    pattern = r'(?:apps|tests|docs|\.sdlc-workflow)/[a-zA-Z0-9_/.-]+'
    return re.findall(pattern, text)
```

**Usage in task creation:**

```python
# In task_create.py:
task_type = detect_task_type(story_id)

StateManager.update_state(task_id, {
    'type': task_type,
    'workflow': TASK_TYPES[task_type],
    'type_detection': 'auto' if 'type' not in story else 'manual'
})

print(f"‚úÖ Detected task type: {task_type}")
if TASK_TYPES[task_type]['skip_phases']:
    print(f"   Skipping phases: {', '.join(TASK_TYPES[task_type]['skip_phases'])}")
```

---

### 2.2 Coverage Delta Intelligence with Validation Agent ‚úÖ APPROVED

**Key Change:** Use refactor-validator agent instead of keyword matching

````python
# MODIFY: .claude/skills/docs-stories/scripts/coverage_delta_check.py

def check_coverage_delta(current: float, baseline: float, task_id: str) -> bool:
    """
    Context-aware coverage delta checking with agent validation for refactors.

    Returns:
        True if coverage change is acceptable, False otherwise
    """

    # Load task context
    state = StateManager.load_state(task_id)
    commit_type = state.get('commit_type', 'feat')

    # Define thresholds by commit type
    thresholds = {
        'feat': 0.0,      # Features must maintain or add coverage
        'fix': 1.0,       # Fixes can drop 1% (removing buggy code)
        'refactor': None, # Requires agent validation (see below)
        'docs': None,     # Skip coverage check for docs
        'test': -10.0,    # Tests can increase coverage significantly
        'chore': 0.5,     # Chores can drop 0.5% (tooling changes)
    }

    allowed_decrease = thresholds.get(commit_type, 0.0)

    # Skip check for certain types
    if allowed_decrease is None and commit_type == 'docs':
        print("‚ÑπÔ∏è Skipping coverage check for docs commit")
        return True

    # Calculate delta
    delta = current - baseline

    # Micro-drops are warnings only
    if delta < 0 and abs(delta) < 0.5:
        print(f"‚ö†Ô∏è Minor coverage decrease: {delta:.2f}%")
        return True

    # Refactor: Requires agent validation
    if commit_type == 'refactor' and delta < 0:
        print(f"‚ö†Ô∏è Refactor decreased coverage by {abs(delta):.2f}%")
        print(f"   Spawning refactor-validator agent to review...")

        # Spawn validation agent
        approved = spawn_refactor_validator(task_id, delta, current, baseline)
        return approved

    # Standard threshold check
    if delta < -abs(allowed_decrease):
        print(f"‚ùå Coverage decreased by {abs(delta):.2f}%")
        print(f"   Allowed for {commit_type}: {allowed_decrease}%")
        return False

    # Warn if suspiciously close to limit (gaming detection)
    if allowed_decrease > 0 and abs(delta - (-allowed_decrease)) < 0.1:
        print(f"‚ö†Ô∏è Coverage delta suspiciously close to limit: {delta:.2f}%")
        print(f"   This may indicate gaming the system - manual review recommended")
        # Don't block, but log for review

    return True


def spawn_refactor_validator(task_id: str, delta: float,
                             current: float, baseline: float) -> bool:
    """
    Spawn specialized agent to validate refactor coverage drop.

    Agent reviews git diff and determines if coverage drop is justified.
    """
    from subprocess import run, PIPE

    # Get git diff
    diff_result = run(['git', 'diff', 'HEAD'], capture_output=True, text=True)
    diff_text = diff_result.stdout

    # Get commit message
    msg_result = run(['git', 'log', '-1', '--pretty=%B'], capture_output=True, text=True)
    commit_message = msg_result.stdout

    # Create validation prompt
    validation_prompt = f"""You are a refactor-validator agent reviewing a code refactor that decreased test coverage.

## Context
- **Coverage Before:** {baseline:.1f}%
- **Coverage After:** {current:.1f}%
- **Delta:** {delta:.2f}%
- **Commit Type:** refactor

## Commit Message
{commit_message}

## Code Changes
{diff_text[:5000]}  # Truncate if too long

## Your Task

Determine if this coverage decrease is justified for a refactor. Valid reasons:

1. **Removed dead code:** Code was unused and its tests were removed
2. **Merged duplicate functions:** Consolidated similar functions, removed redundant tests
3. **Simplified logic:** New implementation is simpler and some edge case tests are no longer needed
4. **Removed defensive code:** Overly defensive checks removed along with their tests

Invalid reasons:
- Developer didn't write tests for refactored code
- Tests were "annoying" so they were removed
- Coverage drop is accidental

## Your Response

Output a JSON object:
{{
  "approved": true/false,
  "reason": "Explanation of why this is/isn't justified",
  "confidence": 0.0-1.0,
  "recommendations": ["List of recommendations if not approved"]
}}

Be rigorous. If in doubt, reject."""

    # Spawn agent (using Task tool or direct subprocess)
    # For now, use direct subprocess to call Claude API
    import json
    import anthropic

    client = anthropic.Anthropic()

    try:
        response = client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1000,
            messages=[{"role": "user", "content": validation_prompt}]
        )

        # Parse response
        response_text = response.content[0].text

        # Extract JSON (handle markdown code blocks)
        if '```json' in response_text:
            json_text = response_text.split('```json')[1].split('```')[0].strip()
        elif '```' in response_text:
            json_text = response_text.split('```')[1].split('```')[0].strip()
        else:
            json_text = response_text.strip()

        validation = json.loads(json_text)

        # Report results
        print(f"\n{'=' * 60}")
        print("REFACTOR VALIDATION RESULTS")
        print(f"{'=' * 60}")
        print(f"Approved: {validation['approved']}")
        print(f"Confidence: {validation['confidence']:.0%}")
        print(f"Reason: {validation['reason']}")

        if not validation['approved'] and validation.get('recommendations'):
            print("\nRecommendations:")
            for rec in validation['recommendations']:
                print(f"  ‚Ä¢ {rec}")

        print(f"{'=' * 60}\n")

        # Log validation to task
        StateManager.update_state(task_id, {
            'refactor_validation': {
                'timestamp': datetime.now().isoformat(),
                'delta': delta,
                'approved': validation['approved'],
                'reason': validation['reason'],
                'confidence': validation['confidence']
            }
        })

        return validation['approved']

    except Exception as e:
        # Agent failed - fall back to asking user
        print(f"‚ö†Ô∏è Refactor-validator agent failed: {e}")
        print(f"   Falling back to user review...")

        print(f"\nRefactor decreased coverage by {abs(delta):.2f}%")
        print(f"Commit message: {commit_message[:200]}")
        print("\nIs this coverage drop justified? (yes/no): ", end='')

        # In practice, this would go through main LLM to ask user
        # For now, return False to block
        return False
````

**Frontend: Simplified Validation**

```python
# Frontend validation: Keep simple per user feedback

def check_frontend_coverage(current: float, baseline: float, task_id: str) -> bool:
    """
    Frontend coverage check with basic automated validation.

    User will manually validate UI work, so keep this simple.
    """

    # Standard coverage check
    if current < baseline - 1.0:
        print(f"‚ùå Frontend coverage decreased by {abs(current - baseline):.2f}%")
        return False

    # Basic automated checks (objective, no expertise needed)
    checks_passed = True

    # 1. Check for missing alt tags
    result = run_basic_a11y_check()
    if result['missing_alt_tags'] > 0:
        print(f"‚ö†Ô∏è Found {result['missing_alt_tags']} images without alt tags")
        checks_passed = False

    # 2. Check for console errors in build
    build_result = run(['npm', 'run', 'build'], capture_output=True, text=True)
    if 'error' in build_result.stderr.lower():
        print(f"‚ö†Ô∏è Build produced console errors")
        checks_passed = False

    # 3. TypeScript errors (already in CI but double-check)
    tsc_result = run(['npm', 'run', 'type-check'], capture_output=True, text=True)
    if tsc_result.returncode != 0:
        print(f"‚ö†Ô∏è TypeScript errors found")
        checks_passed = False

    if not checks_passed:
        print("\n‚ö†Ô∏è Basic frontend validation failed. User review required.")

    return checks_passed


def run_basic_a11y_check() -> dict:
    """Basic accessibility check - just missing alt tags"""
    from pathlib import Path
    import re

    missing_alt_count = 0

    frontend_dir = Path('apps/frontend/src')
    for svelte_file in frontend_dir.rglob('*.svelte'):
        content = svelte_file.read_text()

        # Simple regex for <img without alt
        # Note: This is basic, not comprehensive
        img_tags = re.findall(r'<img[^>]*>', content)
        for tag in img_tags:
            if 'alt=' not in tag:
                missing_alt_count += 1
                print(f"   {svelte_file.name}: <img> missing alt")

    return {'missing_alt_tags': missing_alt_count}
```

---

### 2.3 Parallel CI Validation ‚úÖ APPROVED

**Optimized for cost:**

```yaml
# MODIFY: .github/workflows/sdlc-validation.yml

name: SDLC Validation

on:
  pull_request:
    branches: [main, develop]

jobs:
  # COMBINED: Fast metadata checks (< 1min total)
  validate-metadata:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Validate commit messages
        run: |
          python .claude/skills/docs-stories/scripts/validate_commits.py

      - name: Validate STATE.json
        run: |
          python .claude/skills/docs-stories/scripts/validate_state.py

      - name: Validate ownership
        run: |
          python .claude/skills/docs-stories/scripts/validate_ownership.py

  # PARALLEL: Backend tests
  validate-tests-backend:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          cd apps/server
          pip install -r requirements.txt

      - name: Run backend tests WITHOUT coverage
        run: |
          cd apps/server
          pytest tests/ --tb=short

      - name: Upload test results
        uses: actions/upload-artifact@v3
        with:
          name: backend-test-results
          path: apps/server/.pytest_cache/

  # PARALLEL: Frontend tests
  validate-tests-frontend:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Node
        uses: actions/setup-node@v3
        with:
          node-version: "20"

      - name: Install dependencies
        run: |
          cd apps/frontend
          npm ci

      - name: Run frontend tests WITHOUT coverage
        run: |
          cd apps/frontend
          npm run test

      - name: Upload test results
        uses: actions/upload-artifact@v3
        with:
          name: frontend-test-results
          path: apps/frontend/test-results/

  # SERIAL: Coverage (depends on tests passing)
  validate-coverage:
    needs: [validate-tests-backend, validate-tests-frontend]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Download test results
        uses: actions/download-artifact@v3

      - name: Run coverage check
        run: |
          python .claude/skills/docs-stories/scripts/coverage_delta_check.py

  # FINAL: All checks passed
  final-gate:
    needs: [validate-metadata, validate-coverage]
    runs-on: ubuntu-latest
    steps:
      - name: All validations passed
        run: |
          echo "‚úÖ All SDLC validations passed"
          echo "PR is ready to merge"
```

**Cost analysis:**

- 3 runner jobs (was 7)
- ~6-7 min per PR
- Saves ~40% CI cost

---

## PART 3: Future-Proofing

### 3.1 Task-Parallel Readiness ‚úÖ APPROVED

**STATE.json additions:**

```json
{
  "format_version": "2.1",
  "task_id": "TASK-001",
  "parallel_compatible": true,
  "depends_on": [],
  "blocks": [],
  "story_id": "US-001-auth-login-admin",
  "phase": "IMPLEMENTATION",
  "created_at": "2025-11-01T10:00:00Z"
}
```

**Dependency validation (for future):**

```python
# NEW FILE: .claude/skills/docs-stories/scripts/validate_task_dependencies.py

def validate_dependencies(task_id: str):
    """Validate task dependencies for cycles and existence"""
    state = StateManager.load_state(task_id)
    depends_on = state.get('depends_on', [])

    if not depends_on:
        return  # No dependencies

    # Build dependency graph
    graph = build_dependency_graph()

    # Check for cycles
    if has_cycle(graph, task_id):
        raise ValueError(
            f"{task_id} creates circular dependency.\n"
            f"Dependency chain: {get_cycle_path(graph, task_id)}"
        )

    # Check that dependencies exist and are complete
    for dep_id in depends_on:
        if not task_exists(dep_id):
            raise ValueError(f"Dependency {dep_id} does not exist")

        dep_state = StateManager.load_state(dep_id)
        if dep_state['phase'] != 'COMPLETED':
            print(f"‚ö†Ô∏è Dependency {dep_id} not yet complete (phase: {dep_state['phase']})")


def build_dependency_graph() -> dict:
    """Build graph of all task dependencies"""
    tasks_dir = Path('.claude/tasks')
    graph = {}

    for task_dir in tasks_dir.iterdir():
        if not task_dir.is_dir() or task_dir.name == 'archive':
            continue

        state_file = task_dir / 'STATE.json'
        if state_file.exists():
            state = json.loads(state_file.read_text())
            task_id = state['task_id']
            depends_on = state.get('depends_on', [])
            graph[task_id] = depends_on

    return graph


def has_cycle(graph: dict, start_node: str) -> bool:
    """Detect cycles in dependency graph using DFS"""
    visited = set()
    rec_stack = set()

    def dfs(node):
        visited.add(node)
        rec_stack.add(node)

        for neighbor in graph.get(node, []):
            if neighbor not in visited:
                if dfs(neighbor):
                    return True
            elif neighbor in rec_stack:
                return True

        rec_stack.remove(node)
        return False

    return dfs(start_node)
```

---

### 3.2 Emergency Override with Tech-Lead Agent ‚úÖ APPROVED

**Key Innovation:** Agent-based approval for solo dev workflow

````python
# NEW FILE: .claude/skills/docs-stories/scripts/emergency_override.py

def request_emergency_override(task_id: str,
                               justification: str,
                               category: str) -> bool:
    """
    Request emergency override with tech-lead agent approval.

    For solo developer workflow: Agent acts as objective reviewer.
    For team workflow: Can be extended to require human approval.
    """

    # Validate category
    valid_categories = ['security', 'accessibility', 'production_down', 'data_loss']
    if category not in valid_categories:
        raise ValueError(
            f"Invalid emergency category: {category}\n"
            f"Valid categories: {', '.join(valid_categories)}"
        )

    # Log request
    request_id = log_emergency_request(task_id, justification, category)

    print(f"üö® EMERGENCY OVERRIDE REQUESTED")
    print(f"   Task: {task_id}")
    print(f"   Category: {category}")
    print(f"   Request ID: {request_id}")
    print()
    print("Spawning tech-lead agent for review...")

    # Spawn tech-lead agent
    try:
        approval = spawn_tech_lead_agent(task_id, justification, category)
    except Exception as e:
        # Agent failed - fall back to user
        print(f"‚ö†Ô∏è Tech-lead agent failed: {e}")
        print(f"   Falling back to direct user approval...")
        approval = ask_user_for_emergency_approval(task_id, justification, category)

    if approval['approved']:
        # Log approval
        StateManager.update_state(task_id, {
            'emergency_override': {
                'timestamp': datetime.now().isoformat(),
                'request_id': request_id,
                'category': category,
                'justification': justification,
                'approved_by': 'tech-lead-agent',
                'agent_reasoning': approval['reasoning'],
                'required_validations': approval['required_validations']
            }
        })

        print(f"\n‚úÖ Emergency override APPROVED")
        print(f"   Reasoning: {approval['reasoning']}")
        print(f"   Required validations: {', '.join(approval['required_validations'])}")

        # Run required validations
        run_emergency_validations(task_id, approval['required_validations'])

        return True
    else:
        print(f"\n‚ùå Emergency override REJECTED")
        print(f"   Reasoning: {approval['reasoning']}")
        print(f"   Recommendations:")
        for rec in approval['recommendations']:
            print(f"      ‚Ä¢ {rec}")

        return False


def spawn_tech_lead_agent(task_id: str, justification: str, category: str) -> dict:
    """
    Spawn tech-lead agent to review emergency request.

    Agent evaluates:
    1. Is this truly an emergency?
    2. Are the risks acceptable?
    3. What minimum validation should still run?
    """

    # Load task context
    state = StateManager.load_state(task_id)

    # Create review prompt
    review_prompt = f"""You are a tech-lead agent reviewing an emergency override request.

## Request Details
- **Task ID:** {task_id}
- **Category:** {category}
- **Justification:** {justification}

## Task Context
- **Story ID:** {state.get('story_id')}
- **Phase:** {state.get('phase')}
- **Files Modified:** {len(state.get('files_modified', []))}

## Emergency Categories

**security:** Critical security vulnerability in production
- Examples: XSS, SQL injection, exposed credentials
- Risk: High (data breach possible)
- Validation: Must still run security scan, syntax check

**accessibility:** Accessibility violation blocking users
- Examples: Screen reader broken, keyboard navigation broken
- Risk: Medium (legal liability, user exclusion)
- Validation: Must still run a11y scan, syntax check

**production_down:** Production service is down
- Examples: 500 errors, database connection lost
- Risk: High (revenue loss, user impact)
- Validation: Must still run smoke test, syntax check

**data_loss:** Risk of data loss if not fixed immediately
- Examples: Backup process broken, data corruption in progress
- Risk: Critical (permanent data loss)
- Validation: Must still run data integrity check

## Your Task

Evaluate this request and determine:
1. Is this a genuine emergency in the claimed category?
2. Are the risks of bypassing full validation acceptable?
3. What minimum validations should still run?

## Invalid Emergencies

DO NOT approve for:
- Deadline pressure ("we need to ship today")
- Convenience ("tests are slow")
- Laziness ("don't want to write tests")
- Aesthetic issues ("button color is wrong")

## Your Response

Output a JSON object:
{{
  "approved": true/false,
  "reasoning": "Detailed explanation of your decision",
  "confidence": 0.0-1.0,
  "required_validations": ["list", "of", "validations", "to", "run"],
  "recommendations": ["List of recommendations if rejected"]
}}

Be rigorous. Err on the side of caution. If in doubt, reject."""

    # Call Claude API
    import anthropic
    client = anthropic.Anthropic()

    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1500,
        messages=[{"role": "user", "content": review_prompt}]
    )

    # Parse response
    response_text = response.content[0].text

    # Extract JSON
    if '```json' in response_text:
        json_text = response_text.split('```json')[1].split('```')[0].strip()
    elif '```' in response_text:
        json_text = response_text.split('```')[1].split('```')[0].strip()
    else:
        json_text = response_text.strip()

    approval = json.loads(json_text)

    # Validate response structure
    required_fields = ['approved', 'reasoning', 'confidence', 'required_validations']
    for field in required_fields:
        if field not in approval:
            raise ValueError(f"Agent response missing field: {field}")

    return approval


def log_emergency_request(task_id: str, justification: str, category: str) -> str:
    """Log emergency request to immutable audit log"""
    import uuid
    from filelock import FileLock

    request_id = str(uuid.uuid4())[:8]
    log_file = Path('.claude/emergency_log.jsonl')
    lock_file = Path('.claude/emergency_log.lock')

    entry = {
        'request_id': request_id,
        'timestamp': datetime.now().isoformat(),
        'task_id': task_id,
        'category': category,
        'justification': justification,
        'requester': os.getenv('USER', 'unknown'),
        'approved': None  # Will be updated later
    }

    # Append to log (immutable, append-only)
    lock = FileLock(str(lock_file))
    with lock:
        with open(log_file, 'a') as f:
            f.write(json.dumps(entry) + '\n')

    return request_id


def run_emergency_validations(task_id: str, validations: list):
    """Run minimum required validations"""
    print(f"\nüîç Running emergency validations...")

    validation_scripts = {
        'syntax': '.claude/skills/docs-stories/scripts/validate_syntax.py',
        'security_scan': '.claude/skills/docs-stories/scripts/security_scan.py',
        'a11y_scan': '.claude/skills/docs-stories/scripts/a11y_scan.py',
        'smoke_test': '.claude/skills/docs-stories/scripts/smoke_test.py',
        'data_integrity': '.claude/skills/docs-stories/scripts/validate_data.py'
    }

    for validation in validations:
        if validation in validation_scripts:
            script = validation_scripts[validation]
            print(f"   Running {validation}...")

            result = subprocess.run(['python', script, task_id],
                                  capture_output=True, text=True)

            if result.returncode != 0:
                print(f"   ‚ùå {validation} FAILED")
                print(result.stderr)
                raise ValidationError(f"Emergency validation '{validation}' failed")

            print(f"   ‚úÖ {validation} passed")

    print(f"‚úÖ All emergency validations passed\n")
````

**New Command:**

````markdown
# NEW FILE: .claude/commands/emergency-override.md

You are processing an EMERGENCY override request.

‚ö†Ô∏è **THIS BYPASSES NORMAL VALIDATION** ‚ö†Ô∏è

Use ONLY for genuine emergencies:

- Production down
- Security vulnerability
- Data loss risk
- Accessibility violation blocking users

## Prerequisites

- Task ID must be active
- Justification must be provided
- Category must be specified

## Steps

1. Validate this is a genuine emergency (ask user if unclear)

2. Request override:
   ```bash
   python .claude/skills/docs-stories/scripts/emergency_override.py \
     --task-id {TASK_ID} \
     --category {security|accessibility|production_down|data_loss} \
     --justification "Detailed explanation"
   ```
````

3. Tech-lead agent will review and approve/reject

4. If approved, minimum validations will run

5. If rejected, follow normal workflow

## After Emergency

Once emergency is resolved, create a follow-up task:

- Add missing tests
- Complete full validation
- Document what was bypassed

````

---

## PART 4: Validation Agent System (NEW)

**Core Pattern:** Agent-Based Governance

v2.1 introduces a new architectural pattern: **Validation Agents**

Instead of:
- Complex human approval workflows ‚Üí spawn tech-lead agent
- Blind trust of developer judgment ‚Üí spawn refactor-validator agent
- Hardcoded rules and keywords ‚Üí spawn intelligent review agents

### Validation Agents in v2.1

1. **Tech-Lead Agent** (3.2)
   - Reviews emergency override requests
   - Evaluates risk vs urgency
   - Determines minimum validation requirements
   - Provides objective approval/rejection

2. **Refactor-Validator Agent** (2.2)
   - Reviews code changes when coverage drops on refactor
   - Analyzes if coverage drop is justified
   - Detects gaming attempts
   - Provides structured validation report

3. **Future: Plan-Validator Agent** (not in v2.1)
   - Could review plans for completeness
   - Could detect missing requirements
   - Could suggest improvements

### Benefits

- **Simple for Solo Dev:** Just spawn an agent, no team coordination needed
- **Rigorous Validation:** AI provides objective review, not rubber-stamping
- **Extensible:** Easy to add new validation agents for new scenarios
- **Consistent:** Same rigorous review every time
- **Transparent:** All agent decisions logged and auditable

### Fallback Pattern

All validation agents have fallback:
```python
try:
    approval = spawn_validation_agent(...)
except AgentFailure:
    # Fall back to user approval
    approval = ask_user_directly(...)
````

This ensures workflow never breaks due to agent failures.

---

## Implementation Plan (Updated)

### Phase 1: Foundation (8h)

1. Implement StateManager with filelock (3h)
2. Update all 33 scripts to use StateManager (3h)
3. Add /task-recover command (1h)
4. Testing and validation (1h)

**Deliverables:**

- state_manager.py with locking and backups
- All scripts updated
- Recovery command functional

---

### Phase 2: Critical Fixes (10h)

1. Implement SideEffectTracker (3h)
2. Create undo scripts (git, future migration stubs) (2h)
3. Update SubagentStop hook with retry + rollback (2h)
4. Implement feedback loop limits (2h)
5. Testing (1h)

**Deliverables:**

- side_effect_tracker.py
- undo_git_commit.py (+ future stubs)
- Updated subagent_stop.py
- Updated task-plan.md, task-implement.md

---

### Phase 3: Context Management (4h)

1. Create context-compression.md skill reference (1h)
2. Implement compress_context.py with LLM calls (2h)
3. Test compression with sample tasks (1h)

**Deliverables:**

- .claude/skills/docs-stories/references/context-compression.md
- compress_context.py with Haiku integration

---

### Phase 4: Validation Agents (6h)

1. Implement refactor-validator agent (2h)
2. Implement tech-lead agent (2h)
3. Update coverage_delta_check.py (1h)
4. Create emergency_override.py (1h)

**Deliverables:**

- Refactor-validator agent in coverage_delta_check.py
- Tech-lead agent in emergency_override.py
- /emergency-override command

---

### Phase 5: Optimizations (4h)

1. Implement task type detection (2h)
2. Update task_new.py and phase commands (1h)
3. Setup parallel CI workflow (1h)

**Deliverables:**

- Task type detection in task_new.py
- Updated phase commands to respect skip_phases
- .github/workflows/sdlc-validation.yml

---

### Phase 6: Integration & Testing (4h)

1. End-to-end testing of critical paths (2h)
2. Test validation agents with real scenarios (1h)
3. Test emergency override flow (1h)

**Deliverables:**

- Test suite passing
- All workflows validated
- Edge cases handled

---

### Phase 7: Documentation (2h)

1. Update CLAUDE.md and skill documentation (1h)
2. Create migration guide from v2.0 (1h)

**Deliverables:**

- Updated documentation
- Migration guide

---

**Total:** 38 hours (includes buffer)
**Realistic Timeline:** 5-6 days with testing

---

## Success Criteria

‚úÖ **Reliability**

- Agent failures recoverable 100% of time via retry + rollback
- STATE.json corruption recoverable via backups
- No workflow deadlocks from infinite loops

‚úÖ **Resilience**

- Side effects tracked and rolled back on retry
- Context compression prevents overflow
- Validation agents provide fallback to user approval

‚úÖ **Efficiency**

- Docs task: 5-10min (was 30min)
- Feature task: 30-40min (was 60min)
- CI validation: 6-7min (was 15min)

‚úÖ **Integrity**

- All changes tracked and validated
- Validation agents provide rigorous review
- Emergency overrides logged to immutable audit trail
- "Friction as feature" preserved and strengthened

‚úÖ **Philosophy**

- ‚ùå No bypass mechanisms without agent approval
- ‚ùå No blind trust - agents validate everything
- ‚ùå No complexity - spawn agents, don't build processes
- ‚úÖ Agent-based governance maintains rigor with simplicity

---

## What This Does NOT Change

- ‚ùå No bypass mechanisms for validation (emergency requires agent approval)
- ‚ùå No shortcuts around task context (task types still tracked)
- ‚ùå No weakening of agent boundaries (still enforced)
- ‚ùå No removal of mandatory workflow phases (just optimized per type)

We're making the right path reliable and fast, not creating alternate paths.

---

## Deliverables Summary

### New Files (15)

1. `state_manager.py` - Centralized state management
2. `side_effect_tracker.py` - Side effect tracking
3. `undo_git_commit.py` - Git rollback script
4. `compress_context.py` - LLM-based compression
5. `context-compression.md` - Compression skill reference
6. `emergency_override.py` - Emergency override system
7. `validate_task_dependencies.py` - Dependency validation
8. `task-recover.md` - Recovery command
9. `emergency-override.md` - Emergency command
10. `.claude/emergency_log.jsonl` - Audit log
    11-15. Various validation scripts (syntax, security, a11y, smoke, data)

### Modified Files (38)

- All 33 existing scripts ‚Üí Use StateManager
- `subagent_stop.py` ‚Üí Add retry + rollback
- `coverage_delta_check.py` ‚Üí Add refactor-validator agent
- `task-plan.md` ‚Üí Add feedback loop limits
- `task-implement.md` ‚Üí Add feedback loop limits
- `.github/workflows/sdlc-validation.yml` ‚Üí Parallel jobs

### New Concepts

- **StateManager**: Atomic state updates with locking
- **SideEffectTracker**: Generic side effect rollback
- **Validation Agents**: AI-powered governance
- **Task Types**: Smart workflow optimization
- **LLM Compression**: Context size management

---

**Status:** ‚úÖ Ready for Implementation
**Approval Required From:** Project owner
**Implementation Team:** DevOps + Backend (primary), Frontend (review)
**Testing Required:** Full workflow regression + edge cases
**Rollback Plan:** Revert to v2.0 via git, restore STATE.json backups

---

**Document Version:** 2.1.0
**Last Updated:** 2025-11-01
**Next Review:** After Phase 6 (integration testing)
