# PatternBook SDLC v2.1 Proposal - Architecture Review

**Date:** 2025-11-01
**Reviewers:** dev-backend perspective, dev-frontend perspective, Architecture analysis
**Status:** Comprehensive review with recommendations
**Proposal Version:** v2.1 Critical Fixes & Optimizations

---

## Executive Summary

**Overall Assessment:** ‚úÖ **APPROVED WITH MODIFICATIONS**

The v2.1 proposal correctly identifies critical reliability gaps in v2.0 and proposes sound architectural solutions. The core direction is correct, but several implementation details require refinement before proceeding.

**Key Strengths:**

- StateManager centralization addresses the biggest reliability risk
- Agent retry mechanism fills a critical gap
- Task type optimization is pragmatic and preserves governance
- Context-aware coverage checking is intelligent

**Key Concerns:**

- Side effect rollback not addressed in agent retry
- Implementation timeline appears optimistic
- Compression strategy needs specification
- Emergency override requires stronger controls

**Recommendation:** Approve with the modifications detailed below. Implement in phases with validation gates.

---

## Part 1: Critical Reliability Fixes - Detailed Review

### 1.1 StateManager Resilience ‚úÖ APPROVED

**dev-backend Perspective:**

#### Strengths

- Atomic write pattern with temp files is correct ‚úÖ
- Backup rotation (last 3) is industry standard ‚úÖ
- Schema validation prevents corruption ‚úÖ
- Centralization of 33 scripts' mutations eliminates race conditions ‚úÖ

#### Concerns ‚ö†Ô∏è

**Concurrency Control:**

```python
# CURRENT PROPOSAL:
temp_file.write_text(json.dumps(state, indent=2))
temp_file.replace(state_file)

# RECOMMENDED: Add file locking for parallel task support (future)
import fcntl

def update_state(task_id, updates):
    state_file = Path(f".claude/tasks/{task_id}/STATE.json")

    with open(state_file, 'r+') as f:
        fcntl.flock(f, fcntl.LOCK_EX)  # Exclusive lock
        try:
            state = json.load(f)
            state = deep_merge(state, updates)
            validate_state_schema(state)

            # Atomic write still needed
            temp_file = state_file.with_suffix('.tmp')
            temp_file.write_text(json.dumps(state, indent=2))
            temp_file.replace(state_file)
        finally:
            fcntl.flock(f, fcntl.LOCK_UN)
```

**Rationale:** Section 3.1 mentions task-parallel readiness. Even though parallel tasks are "future," StateManager should be future-proof from day 1. Adding locking now is easier than retrofitting.

**Error Handling:**

```python
# ADD: Exponential backoff for lock contention
MAX_RETRIES = 3
BACKOFF_BASE = 0.1

for attempt in range(MAX_RETRIES):
    try:
        # ... acquire lock and update ...
        break
    except BlockingIOError:
        if attempt == MAX_RETRIES - 1:
            raise
        time.sleep(BACKOFF_BASE * (2 ** attempt))
```

#### Suggestions üí°

1. **Structured Logging:** Log all STATE.json mutations to a separate `.claude/tasks/{task_id}/state.log` file

   - Enables audit trail
   - Helps debug state corruption
   - Format: `{timestamp} | {script} | {field} | {old_value} ‚Üí {new_value}`

2. **Validation Schema:** Create a JSON Schema for STATE.json structure

   ```python
   STATE_SCHEMA = {
       "type": "object",
       "required": ["format_version", "task_id", "phase", "created_at"],
       "properties": {
           "format_version": {"type": "string", "pattern": "^2\\.\\d+$"},
           "task_id": {"type": "string", "pattern": "^TASK-\\d{3}$"},
           # ... complete schema
       }
   }
   ```

3. **Deep Merge Safety:** Ensure `deep_merge()` handles type conflicts
   ```python
   def deep_merge(base, updates):
       """Merge with type safety"""
       for key, value in updates.items():
           if key in base:
               if type(base[key]) != type(value):
                   raise TypeError(f"Type mismatch for {key}: {type(base[key])} vs {type(value)}")
           # ... rest of merge logic
   ```

**Verdict:** ‚úÖ **APPROVED** with above enhancements

---

### 1.2 Agent Failure Recovery ‚ö†Ô∏è NEEDS WORK

**dev-backend Perspective:**

#### Critical Gap: Side Effect Rollback ‚ùå

**Problem:** The proposal handles file checkpoints but ignores other state mutations:

```python
# CURRENT PROPOSAL - INCOMPLETE:
def save_checkpoint(agent_name, context):
    checkpoint_file = Path(f".claude/tasks/{task_id}/checkpoints/{agent_name}-{timestamp}.json")
    checkpoint_file.write_text(json.dumps({
        'partial_work': context['files_modified'],  # ‚úÖ Files tracked
        'last_error': context.get('error'),
        'completed_steps': context.get('steps'),
        'timestamp': timestamp
    }))

# MISSING: What about...
# - Database migrations run via Alembic?
# - Git commits made during implementation?
# - API calls to external services?
# - Docker containers started?
# - Redis keys set?
```

**Scenario that breaks current proposal:**

1. dev-backend agent starts implementing feature
2. Agent runs Alembic migration (creates `users` table)
3. Agent writes code to use `users` table
4. Agent crashes before completion
5. Retry #1 starts
6. Agent tries to run same migration ‚Üí **FAILS** (table exists)
7. Workflow stuck

**Required Solution:**

```python
# NEW: Add side effect tracking to checkpoints
class SideEffectTracker:
    """Track and rollback non-file mutations"""

    def __init__(self, task_id, agent_name):
        self.task_id = task_id
        self.agent_name = agent_name
        self.effects = []

    def record_migration(self, migration_id, direction='upgrade'):
        self.effects.append({
            'type': 'database_migration',
            'migration_id': migration_id,
            'direction': direction,
            'timestamp': now()
        })

    def record_git_commit(self, commit_hash):
        self.effects.append({
            'type': 'git_commit',
            'commit_hash': commit_hash,
            'timestamp': now()
        })

    def record_external_call(self, service, operation, identifier):
        self.effects.append({
            'type': 'external_call',
            'service': service,
            'operation': operation,
            'identifier': identifier,
            'timestamp': now()
        })

    def rollback_all(self):
        """Rollback in reverse order"""
        for effect in reversed(self.effects):
            if effect['type'] == 'database_migration':
                # Run alembic downgrade
                run_alembic_downgrade(effect['migration_id'])
            elif effect['type'] == 'git_commit':
                # Soft reset (keep files but undo commit)
                run_git_reset_soft(f"{effect['commit_hash']}^")
            elif effect['type'] == 'external_call':
                # Log warning (external calls often can't be undone)
                log_warning(f"Cannot rollback external call: {effect}")

# MODIFY: SubagentStop hook to use tracker
def main():
    # ... existing validation ...

    if not is_work_complete(agent_context):
        retry_count = state.get('agent_retries', {}).get(agent_name, 0)

        if retry_count < 3:
            # NEW: Rollback side effects before retry
            tracker = load_side_effect_tracker(task_id, agent_name)
            tracker.rollback_all()

            # ... rest of retry logic ...
```

**Integration with Scripts:**

All scripts that cause side effects must use the tracker:

```python
# MODIFY: .claude/skills/docs-stories/scripts/run_migrations.py
from side_effect_tracker import SideEffectTracker

def run_migrations(task_id):
    tracker = SideEffectTracker.load(task_id)

    result = subprocess.run(['alembic', 'upgrade', 'head'])
    if result.returncode == 0:
        # Extract migration ID from output
        migration_id = extract_migration_id(result.stdout)
        tracker.record_migration(migration_id)
        tracker.save()
```

**Verdict:** ‚ùå **BLOCKER** - Must add side effect rollback before implementing agent retry

---

#### Other Concerns with Agent Retry

**1. Retry Count Storage:**
Current proposal stores retries in STATE.json, but what if STATE.json corruption causes the failure?

**Solution:** Store retry metadata separately

```python
retry_file = Path(f".claude/tasks/{task_id}/agent_retries.json")
# Separate from STATE.json to avoid chicken-egg problem
```

**2. Idempotency:**
Agents must be designed to be idempotent. If retry #2 runs, it should safely handle:

- Files already modified from attempt #1
- Partial database state
- Git commits in progress

**Recommendation:** Add to agent prompt template:

```markdown
## Idempotency Requirements

Your implementation must be safe to run multiple times:

- Check if files already have expected changes before editing
- Use `CREATE TABLE IF NOT EXISTS` patterns
- Skip operations if their result already exists
- Use git status checks before committing
```

**3. Checkpoint Restoration:**
How does retry #2 know what retry #1 already completed?

**Solution:** Load checkpoint before retry

```python
def restore_from_checkpoint(task_id, agent_name):
    """Load partial work for retry"""
    latest_checkpoint = find_latest_checkpoint(task_id, agent_name)
    if latest_checkpoint:
        context = json.loads(latest_checkpoint.read_text())
        return f"""
        Previous attempt failed. Completed steps:
        {context['completed_steps']}

        Error encountered:
        {context['last_error']}

        Files modified (may be partial):
        {context['files_modified']}

        Resume work from where it failed, being careful to check
        if any operations already completed.
        """
    return None
```

---

### 1.3 Feedback Loop Limits ‚úÖ APPROVED

**dev-backend Perspective:**

#### Strengths

- MAX_PLANNING_ITERATIONS = 5 is reasonable ‚úÖ
- Auto-approval with warning prevents deadlock ‚úÖ
- Logged to STATE.json for audit trail ‚úÖ

#### Suggestions üí°

**1. User Override:**
Allow user to extend limit if genuinely needed:

```python
if iteration >= MAX_PLANNING_ITERATIONS:
    print(f"‚ö†Ô∏è Planning iteration limit reached ({MAX_PLANNING_ITERATIONS})")
    print("Options:")
    print("  1. Auto-approve current plan (RECOMMENDED)")
    print("  2. Continue with 3 more iterations")
    print("  3. Abort and revise story requirements")

    # In practice, prompt would go to main LLM to ask user
    choice = ask_user_choice()
```

**2. Progressive Warnings:**
Don't surprise user at iteration 5:

```python
if iteration == 3:
    print("üìä Planning feedback: 3/5 iterations used")
elif iteration == 4:
    print("‚ö†Ô∏è Planning feedback: 4/5 iterations used (1 remaining)")
elif iteration >= 5:
    print("üõë Planning feedback: Limit reached")
```

**3. Same Pattern for Implementation:**
Proposal mentions task-implement but doesn't show limits. Should be consistent:

```python
MAX_IMPLEMENTATION_ITERATIONS = 3  # Lower than planning
# Implementation feedback should be rarer
```

**Verdict:** ‚úÖ **APPROVED** with suggestions

---

### 1.4 Context Size Management ‚ö†Ô∏è NEEDS SPECIFICATION

**dev-backend Perspective:**

#### Critical Questions

**1. Compression Strategy:** Lossy or lossless?

**Current proposal is ambiguous:**

```python
if token_count > MAX_CONTEXT_TOKENS:
    context = compress_context(context, target_tokens=MAX_CONTEXT_TOKENS * 0.8)
```

**Need clarity:**

```python
def compress_context(context, target_tokens):
    """
    SPECIFY: What actually happens here?

    Option A (Lossless): Truncate least important items
    - Remove older issues from recent_issues
    - Truncate file_modified lists
    - Risk: Lose important context

    Option B (Lossy): Summarize with LLM
    - Use fast model to summarize verbose sections
    - Risk: Hallucination, cost

    Option C (Hybrid): Structure-aware compression
    - Keep critical fields (story, acceptance_criteria) intact
    - Summarize verbose fields (plan details, test results)
    - Truncate repetitive data (commit lists, file lists)
    """
    # Which approach? Document it!
```

**Recommendation:** Use **Option C (Hybrid)** with clear rules:

```python
class ContextManager:
    # Critical: Never compressed (hard limit violations = error)
    ESSENTIAL_FIELDS = ['story', 'acceptance_criteria', 'current_phase']

    # Important: Truncate but keep structure
    TRUNCATE_FIELDS = ['plan', 'test_results', 'commits']
    TRUNCATE_LIMITS = {'plan': 1000, 'test_results': 500, 'commits': 10}

    # Optional: Drop if needed
    OPTIONAL_FIELDS = ['previous_errors', 'git_history']

    def compress_context(self, context, target_tokens):
        compressed = {}

        # Phase 1: Keep all essentials
        for field in self.ESSENTIAL_FIELDS:
            compressed[field] = context[field]

        # Phase 2: Truncate important fields
        for field in self.TRUNCATE_FIELDS:
            compressed[field] = self.truncate(
                context[field],
                self.TRUNCATE_LIMITS[field]
            )

        # Phase 3: Check token count
        if estimate_tokens(compressed) > target_tokens:
            # Drop optional fields one by one
            for field in self.OPTIONAL_FIELDS:
                if estimate_tokens(compressed) <= target_tokens:
                    break
                compressed.pop(field, None)

        # Phase 4: Last resort - error if still over
        if estimate_tokens(compressed) > target_tokens * 1.1:
            raise ContextTooLargeError(
                f"Cannot compress to {target_tokens} tokens. "
                f"Essential fields alone are {estimate_tokens(compressed)} tokens. "
                f"Story may be too complex - consider splitting."
            )

        return compressed
```

**2. Token Estimation Accuracy:**

```python
def estimate_tokens(text):
    """How accurate is this?"""
    # Rough heuristic: 1 token ‚âà 4 characters
    # BUT: Code has different token density than prose
    # AND: JSON structure adds overhead

    # RECOMMEND: Use tiktoken library for accuracy
    import tiktoken
    enc = tiktoken.encoding_for_model("claude-sonnet-4")
    return len(enc.encode(str(text)))
```

**dev-frontend Perspective:**

#### UI-Specific Context Needs

**Problem:** Frontend work often needs visual context that doesn't compress well:

- Design mockups (base64 encoded images)
- CSS/styling requirements
- Accessibility requirements
- Component API specifications

**Recommendation:** Frontend-specific context priorities:

```python
# MODIFY: ContextManager.prepare_context()
def prepare_context(task_id, phase):
    essential = {
        'PLANNING': ['story', 'acceptance_criteria'],
        'IMPLEMENTATION': {
            'backend': ['story', 'plan', 'acceptance_criteria', 'api_spec'],
            'frontend': ['story', 'plan', 'acceptance_criteria', 'design_spec', 'component_api']
        },
        # ... rest
    }

    # Agent-aware context loading
    agent = get_current_agent()
    if agent == 'dev-frontend':
        # Frontend needs design context more than git history
        context['design_spec'] = load_design_spec(task_id)
        # Drop git_history if needed for space
    elif agent == 'dev-backend':
        # Backend needs API specs and database schema
        context['api_spec'] = load_api_spec(task_id)
        context['db_schema'] = load_db_schema()
```

**Verdict:** ‚ö†Ô∏è **NEEDS WORK** - Specify compression strategy before implementation

---

## Part 2: Efficiency Optimizations - Detailed Review

### 2.1 Task Types for Smart Workflow ‚úÖ APPROVED

**dev-frontend Perspective:**

#### Strengths

- Detection heuristics look solid ‚úÖ
- File pattern matching will catch most cases ‚úÖ
- Fallback to 'fullstack' is safe ‚úÖ

#### Concerns ‚ö†Ô∏è

**1. Edge Cases:**

````python
# SCENARIO 1: Documentation with code examples
# File: docs/api-guide.md contains:
"""
## Example Usage
```python
from bestays import Client
client = Client()
````

"""

# Detection might see "Client" keyword and think it's backend code

# SOLUTION: Check file path FIRST before analyzing content

def detect_task_type(story_id):
story = load_story(story_id)

    # Priority 1: Explicit tags
    if 'type:docs' in story.get('tags', []):
        return 'docs'

    # Priority 2: File paths in acceptance criteria
    criteria = story.get('acceptance_criteria', '')
    files_mentioned = extract_file_paths(criteria)

    if all(f.startswith('docs/') for f in files_mentioned):
        return 'docs'

    # Priority 3: Content analysis
    # ... existing heuristics

````

**2. Missing Type: Component Library Work**

```python
TASK_TYPES = {
    # ... existing types ...

    'component': {  # NEW
        'patterns': ['apps/frontend/src/lib/components/**'],
        'skip_phases': [],
        'required_agents': ['dev-frontend'],
        'validation': ['vitest', 'storybook', 'a11y']  # Component-specific
    }
}
````

**Rationale:** Building reusable components has different workflow than features:

- Needs Storybook examples
- Needs more accessibility testing
- Often doesn't need backend at all

**3. Override Mechanism:**

```python
# ADD: Manual override in story frontmatter
---
title: Add user authentication
type: fullstack  # Override auto-detection
---

# In task_new.py:
def detect_task_type(story_id):
    story = load_story(story_id)

    # Priority 0: Manual override
    if 'type' in story:
        return story['type']

    # ... rest of detection
```

**dev-frontend DX Impact:** üé® **POSITIVE**

This change will significantly improve frontend workflow:

- Docs-only changes: 30min ‚Üí 5min (no backend agent spawned)
- Component work: Optimized validation (Storybook + a11y)
- UI-only features: Skip backend research phase

**Verdict:** ‚úÖ **APPROVED** with edge case handling and override mechanism

---

### 2.2 Coverage Delta Intelligence ‚ö†Ô∏è NEEDS REFINEMENT

**dev-backend Perspective:**

#### Concerns

**1. Refactor Abuse Potential:**

```python
# CURRENT PROPOSAL:
if commit_type == 'refactor':
    allowed_decrease = 5.0  # Too permissive?

# SCENARIO:
# Developer commits:
# "refactor(auth): simplify login flow"
# Removes 200 lines of code
# Coverage drops from 85% ‚Üí 81% (4% drop)
# PASSES ‚úÖ

# But wait - did they actually refactor or just delete tests?
```

**Solution:** Require justification

```python
def check_coverage_delta(current, baseline, context):
    commit_type = extract_commit_type()

    if commit_type == 'refactor':
        allowed_decrease = 5.0
        delta = current - baseline

        if delta < 0:
            # Refactor decreased coverage - needs explanation
            commit_message = get_commit_message()

            # Check for justification keywords
            justification_present = any(keyword in commit_message.lower() for keyword in [
                'removed dead code',
                'merged duplicates',
                'simplified logic',
                'consolidated',
                'coverage drop justified'
            ])

            if not justification_present and abs(delta) > 1.0:
                print(f"‚ùå Refactor decreased coverage by {abs(delta):.1f}%")
                print(f"   Commit message must explain why:")
                print(f"   - 'removed dead code'")
                print(f"   - 'merged duplicate functions'")
                print(f"   - 'simplified logic' (if tests redundant)")
                return False

    # ... rest of logic
```

**2. Gaming Detection:**

Monitor for suspicious patterns:

```python
# RED FLAG: Coverage delta exactly at limit
if abs(delta - allowed_decrease) < 0.1:
    print(f"‚ö†Ô∏è Coverage delta suspiciously close to limit: {delta:.2f}%")
    print(f"   Manual review recommended")
    # Don't block, but flag for review
```

**3. Integration with Existing Script:**

```python
# MODIFY: .claude/skills/docs-stories/scripts/coverage_delta_check.py
# Current v2.0 script probably looks like:

def check_coverage(current, baseline):
    if current < baseline:
        return False
    return True

# v2.1 upgrade:
def check_coverage(current, baseline, task_id):
    """Enhanced with context-aware checking"""

    # Load task context
    state = load_state(task_id)
    commit_type = state.get('commit_type', 'feat')

    # Apply context-aware thresholds
    thresholds = {
        'feat': 0.0,      # Features must add coverage
        'fix': 1.0,       # Fixes can drop 1%
        'refactor': 5.0,  # Refactor can drop 5% with justification
        'docs': None,     # Skip for docs
        'test': -5.0,     # Tests can INCREASE coverage without code
    }

    allowed = thresholds.get(commit_type, 0.0)

    if allowed is None:  # Skip check
        return True

    delta = current - baseline

    # ... rest of enhanced logic
```

**dev-frontend Perspective:**

#### UI Testing Concerns

**Problem:** Component coverage is different from backend coverage:

- Visual regression tests don't count as "coverage"
- Interaction tests are harder to measure
- Snapshot tests inflate coverage without testing behavior

**Recommendation:** Different metrics for frontend

```python
# ADD: Frontend-specific validation
TASK_TYPES = {
    'frontend': {
        # ... existing ...
        'validation': [
            'vitest',            # Unit test coverage
            'a11y',              # Accessibility score
            'visual-regression'  # Visual diff count
        ],
        'coverage_threshold': 70  # Lower than backend (80)
    }
}

# Frontend coverage checking:
def check_frontend_coverage(current, baseline, task_id):
    # Standard coverage check
    coverage_ok = current >= baseline

    # But also check accessibility
    a11y_score = run_a11y_check()
    a11y_ok = a11y_score >= 90  # WCAG AA minimum

    # And visual regressions
    visual_diffs = run_visual_regression()
    visual_ok = len(visual_diffs) == 0 or user_approved(visual_diffs)

    return coverage_ok and a11y_ok and visual_ok
```

**Verdict:** ‚ö†Ô∏è **NEEDS REFINEMENT** - Add justification requirement, gaming detection, frontend-specific metrics

---

### 2.3 Parallel CI Validation ‚úÖ APPROVED

**dev-backend Perspective:**

#### Strengths

- Dependency graph is correct ‚úÖ
- Splitting jobs will significantly speed up CI ‚úÖ
- final-gate pattern is industry standard ‚úÖ

#### Concerns ‚ö†Ô∏è

**1. Cost Implications:**

```yaml
# CURRENT PROPOSAL:
jobs:
  validate-commits: # 1 runner, ~30s
  validate-state: # 1 runner, ~30s
  validate-tests: # 2 runners (matrix), ~5min each
  validate-coverage: # 1 runner, ~1min
  validate-ownership: # 1 runner, ~30s
  final-gate: # 1 runner, ~10s

# Total: 7 runner jobs
# GitHub Actions: 2000 free minutes/month
# Estimate: ~6 min per PR √ó 7 runners = 42 minutes billed per PR
# ~47 PRs per month before hitting limit
```

**Recommendation:** Add cost optimization:

```yaml
# Combine lightweight checks into single job
jobs:
  validate-metadata: # NEW: Combines commits + state + ownership
    runs-on: ubuntu-latest
    steps:
      - name: Check commits
      - name: Check STATE.json
      - name: Check ownership
    # These are fast (<1min total), no need to parallelize

  validate-tests-backend: # Separate (slow)
  validate-tests-frontend: # Separate (slow)

  validate-coverage: # Still depends on tests
  final-gate: # Still depends on all

# New total: 5 runner jobs (saves 30% cost)
```

**2. Coverage Calculation:**

Should coverage run in parallel or depend on tests?

```yaml
# OPTION A (Current): Serial dependency
validate-tests:
  # runs tests
validate-coverage:
  needs: validate-tests
  # calculates coverage from test results

# OPTION B: Parallel (coverage runs tests again)
validate-tests:
  # runs tests WITHOUT coverage (faster)
validate-coverage:
  # runs tests WITH coverage (slower but complete)
  # Both run in parallel

# RECOMMENDATION: Option A (current proposal)
# Running tests twice wastes CI time
```

**3. Race Conditions:**

```yaml
# POTENTIAL ISSUE:
validate-tests:
  strategy:
    matrix:
      component: [backend, frontend]

# What if both try to write to same artifact?
# SOLUTION: Separate artifacts
- uses: actions/upload-artifact@v3
  with:
    name: test-results-{{ matrix.component }}  # Unique names
```

**Verdict:** ‚úÖ **APPROVED** with cost optimization

---

## Part 3: Future-Proofing - Detailed Review

### 3.1 Task-Parallel Readiness ‚úÖ APPROVED

**dev-backend Perspective:**

#### Strengths

- Minimal changes, good design ‚úÖ
- depends_on/blocks fields are forward-thinking ‚úÖ
- CSV format for current.txt is simple ‚úÖ

#### Suggestions üí°

**1. Dependency Validation:**

```python
# When task-parallel is actually implemented, need cycle detection:
def validate_task_dependencies(task_id):
    """Prevent circular dependencies"""
    state = load_state(task_id)
    depends_on = state.get('depends_on', [])

    # Build dependency graph
    graph = build_dependency_graph()

    # Check for cycles
    if has_cycle(graph, task_id):
        raise ValueError(f"{task_id} creates circular dependency")

    # Check that dependencies exist
    for dep in depends_on:
        if not task_exists(dep):
            raise ValueError(f"Dependency {dep} does not exist")
```

**2. Concurrent STATE.json Updates:**

This is why StateManager needs file locking (section 1.1):

```python
# SCENARIO: Two parallel tasks
# TASK-001: Updates .claude/tasks/TASK-001/STATE.json
# TASK-002: Updates .claude/tasks/TASK-002/STATE.json
# ‚úÖ No conflict - separate files

# But what about:
# .claude/tasks/current.txt contains "TASK-001,TASK-002"
# Both try to update it simultaneously
# ‚ùå Race condition

# SOLUTION: Atomic current.txt updates
import fcntl

def set_current_tasks(task_ids):
    current_file = Path(".claude/tasks/current.txt")
    with open(current_file, 'w') as f:
        fcntl.flock(f, fcntl.LOCK_EX)
        f.write(','.join(task_ids))
        fcntl.flock(f, fcntl.LOCK_UN)
```

**Verdict:** ‚úÖ **APPROVED** with dependency validation

---

### 3.2 Emergency Override System ‚ö†Ô∏è NEEDS CONTROLS

**dev-backend Perspective:**

#### Critical Concerns

**1. Approval Mechanism Too Vague:**

```python
# CURRENT PROPOSAL:
if not confirm_emergency():
    return

# What is confirm_emergency()?
# Who approves?
# How is it logged?
```

**Required Specification:**

```python
def emergency_override(task_id, justification):
    """Requires multi-person approval"""

    # Step 1: Record request
    request_id = log_emergency_request(task_id, justification, requester=get_user())

    # Step 2: Require approval from different person
    print(f"üö® Emergency override requested for {task_id}")
    print(f"   Justification: {justification}")
    print(f"   Request ID: {request_id}")
    print()
    print("Approval required from team lead or project manager.")
    print(f"Run: claude emergency-approve {request_id}")
    print()

    # Block until approved
    while not is_approved(request_id):
        time.sleep(5)

    # Step 3: Log approval
    StateManager.update_state(task_id, {
        'emergency_override': {
            'timestamp': now(),
            'justification': justification,
            'requester': get_user(),
            'approver': get_approver(request_id),
            'request_id': request_id,
            'validations_skipped': get_skipped_validations()
        }
    })

    # Step 4: Still run minimal validation
    run_emergency_validation()  # Syntax, security only
```

**2. Audit Trail:**

```python
# MANDATORY: Log all emergency overrides to immutable log
def log_emergency_request(task_id, justification, requester):
    log_file = Path(".claude/emergency_log.jsonl")  # Append-only

    entry = {
        'request_id': generate_uuid(),
        'timestamp': now(),
        'task_id': task_id,
        'requester': requester,
        'justification': justification,
        'approved': False,
        'approver': None
    }

    # Append-only log
    with open(log_file, 'a') as f:
        fcntl.flock(f, fcntl.LOCK_EX)
        f.write(json.dumps(entry) + '\n')
        fcntl.flock(f, fcntl.LOCK_UN)

    return entry['request_id']

# Monthly audit report
def generate_emergency_audit():
    """Report all emergency overrides for review"""
    # Helps identify abuse patterns
```

**dev-frontend Perspective:**

#### UI Emergency Scenarios

**Valid emergency cases:**

- Production CSS broken (site unusable)
- Accessibility violation blocking screen readers
- Security issue in frontend auth
- XSS vulnerability

**Invalid cases (should NOT bypass):**

- "Design needs to ship today"
- "I don't want to write tests"
- "Coverage check is annoying"

**Recommendation:** Categorize emergencies

```python
EMERGENCY_CATEGORIES = {
    'security': {
        'allowed': True,
        'min_validation': ['syntax', 'security_scan', 'xss_check'],
        'approval_required': 'security_lead'
    },
    'accessibility': {
        'allowed': True,
        'min_validation': ['syntax', 'a11y_scan'],
        'approval_required': 'frontend_lead'
    },
    'production_down': {
        'allowed': True,
        'min_validation': ['syntax', 'smoke_test'],
        'approval_required': 'tech_lead'
    },
    'deadline': {  # NOT a valid emergency
        'allowed': False,
        'reason': 'Deadlines do not bypass quality. Plan better next time.'
    }
}

def emergency_override(task_id, justification, category):
    if category not in EMERGENCY_CATEGORIES:
        raise ValueError(f"Invalid emergency category: {category}")

    policy = EMERGENCY_CATEGORIES[category]

    if not policy['allowed']:
        raise PermissionError(policy['reason'])

    # ... require approval from designated person ...
    # ... run minimum validation for category ...
```

**Verdict:** ‚ö†Ô∏è **NEEDS CONTROLS** - Multi-person approval, audit trail, categorization

---

## Implementation Timeline Assessment

**Proposal Estimate:** 16 hours over 4 days

**Realistic Assessment:** ‚ö†Ô∏è **20-24 hours over 5-6 days**

### Detailed Breakdown

| Phase                                      | Proposal | Realistic | Risk Factors                                      |
| ------------------------------------------ | -------- | --------- | ------------------------------------------------- |
| **Phase 1: Critical Fixes (StateManager)** | 4h       | 6h        | File locking edge cases, testing across platforms |
| **Phase 2: Critical Fixes (Agent Retry)**  | 4h       | 6-8h      | **Blocker**: Side effect rollback adds complexity |
| **Phase 3: Optimizations (Task Types)**    | 6h       | 5h        | Straightforward implementation                    |
| **Phase 4: Future-Proofing**               | 2h       | 3h        | Emergency override needs careful design           |
| **Total**                                  | 16h      | 20-24h    |                                                   |

### Risk Assessment

**High Risk (Likely to extend timeline):**

- Agent retry side effect rollback (new requirement, complex)
- Context compression strategy (needs specification)
- Emergency override approval flow (needs UX design)

**Medium Risk:**

- File locking cross-platform testing (Windows vs Unix)
- Coverage delta justification parsing (needs ML?)
- Task type edge case handling

**Low Risk:**

- Feedback loop limits (straightforward)
- Parallel CI (standard pattern)
- Task-parallel readiness (minimal changes)

### Recommendation

**Phase Implementation:**

1. **Week 1: Foundation (8h)**

   - StateManager with file locking
   - Side effect tracker design
   - Testing and validation

2. **Week 2: Reliability (8h)**

   - Agent retry with rollback
   - Context management with specified strategy
   - Feedback loop limits

3. **Week 3: Optimization (6h)**

   - Task type detection and optimization
   - Coverage delta intelligence
   - Parallel CI setup

4. **Week 4: Future-Proofing (2h + buffer)**
   - Emergency override with controls
   - Documentation
   - Integration testing

**Total: 24 hours + 4 hour buffer = 28 hours over 4 weeks**

---

## Summary of Recommendations

### ‚úÖ Approved as Proposed

- StateManager concept (add file locking)
- Feedback loop limits
- Task type optimization (add override mechanism)
- Parallel CI (optimize for cost)
- Task-parallel readiness (add dependency validation)

### ‚ö†Ô∏è Approved with Modifications

- **Agent Retry:** Add side effect rollback (CRITICAL)
- **Context Management:** Specify compression strategy
- **Coverage Delta:** Add justification requirement and frontend metrics
- **Emergency Override:** Add multi-person approval and audit trail

### ‚ùå Blockers (Must Address Before Implementation)

1. **Side Effect Rollback** - Agent retry is incomplete without this
2. **Compression Strategy** - Cannot implement context manager without spec
3. **Emergency Approval Flow** - Security risk without proper controls

### üí° Additional Suggestions

1. Structured logging for STATE.json mutations
2. JSON Schema validation for STATE.json
3. Progressive warnings for iteration limits
4. Component task type for frontend work
5. Frontend-specific context priorities
6. Cost optimization for CI
7. Emergency categorization system

---

## Questions for User

Before proceeding with implementation, please clarify:

1. **Side Effect Rollback:** Are database migrations, git commits, and external API calls expected during agent execution? If yes, rollback mechanism is mandatory.

2. **Context Compression:** Prefer lossless (truncation) or lossy (LLM summarization)? Cost/quality tradeoff.

3. **Emergency Override:** Who should approve emergencies? Same person, tech lead, two-person rule?

4. **Timeline:** Accept 20-24 hour realistic estimate or prefer 16 hour optimistic? Affects planning.

5. **Coverage Delta:** Should refactor commits require explicit justification in message? Or trust developer judgment?

6. **Frontend Metrics:** Include accessibility scores and visual regression in validation? Or coverage only?

7. **Task Types:** Add 'component' type for component library work? Or fold into 'frontend'?

---

## Changelog Preview

If approved with modifications, the v2.1 changelog should include:

### Added

- StateManager with atomic writes, backups, file locking, structured logging
- Agent retry with side effect rollback (DB, git, external calls)
- Checkpoint system for partial work recovery
- Feedback loop limits (planning: 5, implementation: 3)
- Context size management with hybrid compression
- Task type detection and workflow optimization (6 types)
- Coverage delta intelligence with context-aware thresholds
- Parallel CI validation with cost optimization
- Task-parallel compatibility (STATE.json structure)
- Emergency override with multi-person approval

### Modified

- coverage_delta_check.py: Context-aware thresholds by commit type
- SubagentStop hook: Agent retry mechanism
- All 33 scripts: Use StateManager instead of direct JSON writes
- CI workflow: Split into parallel jobs
- Task types: Added override mechanism via story frontmatter

### Fixed

- STATE.json single point of failure ‚Üí backups and atomic writes
- Agent crashes leave workflow in limbo ‚Üí retry with rollback
- Infinite feedback loops ‚Üí iteration limits
- Context overflow ‚Üí intelligent compression
- Rigid workflow for all changes ‚Üí task type optimization
- Coverage delta too strict ‚Üí context-aware checking
- Sequential CI is slow ‚Üí parallel validation

### Security

- Emergency override requires multi-person approval
- Audit log for all emergency uses
- File locking prevents concurrent corruption

---

**Document Status:** ‚úÖ Complete
**Next Step:** User reviews and answers questions
**Implementation:** Blocked pending clarifications
